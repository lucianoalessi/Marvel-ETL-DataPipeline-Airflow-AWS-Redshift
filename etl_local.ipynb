{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marvel ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fastparquet\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of necessary functions for the ETL:\n",
    "\n",
    "In this code block, the functions that will be used in the ETL (Extraction, Transformation, and Loading) process of the data from the Marvel API are defined. Each function is documented for its use and has its own responsibility within the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api_connection(endpoint, params):\n",
    "    \"\"\"\n",
    "    Tests the connection to an API endpoint by sending a GET request and checks the connection status.\n",
    "\n",
    "    Parameters:\n",
    "    endpoint (str): The full URL of the API endpoint to make the request to.\n",
    "    params (dict): A dictionary with the parameters to send in the GET request.\n",
    "\n",
    "    Returns:\n",
    "    bool: Returns `True` if the connection to the API was successful (status code 200), otherwise returns `False`.\n",
    "\n",
    "    Exceptions:\n",
    "    Catches any exception related to HTTP requests via `requests.RequestException` and returns `False`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully connected to the Marvel API.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error connecting to the Marvel API: {response.status_code} - {response.reason}\")\n",
    "            return False\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Connection error to the Marvel API: {e}\")\n",
    "        return False    \n",
    "\n",
    "def get_data(base_url, endpoint, params=None):\n",
    "    \"\"\"\n",
    "    Performs a paginated GET request to an API to retrieve all available data.\n",
    "\n",
    "    Parameters:\n",
    "    base_url (str): The base URL of the API.\n",
    "    endpoint (str): The specific endpoint of the API to make the request to.\n",
    "    params (dict, optional): The request parameters, including pagination (limit and offset).\n",
    "\n",
    "    Returns:\n",
    "    list: A list with all the results retrieved from the API.\n",
    "    None: If an error occurs during the request.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_results = []\n",
    "        limit = 100  # Maximum number of results per page allowed by the API\n",
    "        offset = 0   # Pagination starts from the first result\n",
    "        \n",
    "        while True:\n",
    "            # Update the 'offset' and 'limit' parameters in each request\n",
    "            params.update({'limit': limit, 'offset': offset})\n",
    "            response = requests.get(f\"{base_url}/{endpoint}\", params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            results = data['data']['results']\n",
    "            all_results.extend(results)  # Store the retrieved results\n",
    "            \n",
    "            # Check if there are more results to fetch\n",
    "            total_results = data['data']['total']\n",
    "            if len(all_results) >= total_results:\n",
    "                break\n",
    "            \n",
    "            # Update the offset to retrieve the next page of results\n",
    "            offset += limit\n",
    "        \n",
    "        return all_results\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_comics(base_url, endpoint, params=None):\n",
    "    \"\"\"\n",
    "    Performs a paginated GET request to the API to retrieve comic data, limiting the total to 500 results.\n",
    "    This limit of 500 is set to simplify the practice and testing of the data pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    base_url (str): The base URL of the API.\n",
    "    endpoint (str): The specific endpoint of the API for comics.\n",
    "    params (dict, optional): The request parameters, including pagination (limit and offset).\n",
    "\n",
    "    Returns:\n",
    "    list: A list with the results retrieved from the API, limited to 1000 items.\n",
    "    None: If an error occurs during the request.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_results = []\n",
    "        limit = 100  # Maximum number of results per page allowed by the API\n",
    "        offset = 0   # Pagination starts from the first result\n",
    "        \n",
    "        while True:\n",
    "            # Update the 'offset' and 'limit' parameters in each request\n",
    "            params.update({'limit': limit, 'offset': offset})\n",
    "            response = requests.get(f\"{base_url}/{endpoint}\", params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            results = data['data']['results']\n",
    "            all_results.extend(results)  # Store the retrieved results\n",
    "            \n",
    "            # Check if there are more results to fetch\n",
    "            total_results = data['data']['total']\n",
    "            #if len(all_results) >= total_results:\n",
    "            if len(all_results) >= 500:\n",
    "                break\n",
    "            \n",
    "            # Update the offset to retrieve the next page of results\n",
    "            offset += limit\n",
    "        \n",
    "        return all_results\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    \n",
    "def build_table(json_data, record_path=None):\n",
    "    \"\"\"\n",
    "    Builds a pandas DataFrame from JSON formatted data.\n",
    "\n",
    "    Parameters:\n",
    "    json_data (dict/list): The data in JSON format obtained from an API.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the data, or None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.json_normalize(json_data, record_path)\n",
    "        return df\n",
    "    except ValueError:\n",
    "        print(\"The data is not in the expected format\")\n",
    "        return None\n",
    "\n",
    "def save_to_parquet(df, output_path, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame in Parquet format in the specified directory using fastparquet.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to save.\n",
    "    path (str): The file path where the DataFrame will be saved.\n",
    "    partition_cols (list, optional): Columns by which the data will be partitioned.\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(output_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    df.to_parquet(output_path, partition_cols=partition_cols, engine='fastparquet')\n",
    "\n",
    "def get_last_extraction_date(file_path):\n",
    "    \"\"\"\n",
    "    Retrieves the date of the last incremental extraction from a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The file path that contains the last extraction date.\n",
    "\n",
    "    Returns:\n",
    "    datetime: The date of the last extraction, or None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            last_extraction_date = file.readline().strip()\n",
    "            return datetime.strptime(last_extraction_date, '%Y-%m-%d')\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        return None\n",
    "\n",
    "def update_last_extraction_date(file_path, extraction_date):\n",
    "    \"\"\"\n",
    "    Updates the date of the last incremental extraction in a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The file path where the extraction date will be saved.\n",
    "    extraction_date (datetime): The date of the current extraction.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(extraction_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "# def get_data(base_url, endpoint, params=None):\n",
    "#     try:\n",
    "#         all_results = []\n",
    "       \n",
    "#         while True:\n",
    "#             response = requests.get(f\"{base_url}/{endpoint}\", params=params)\n",
    "#             response.raise_for_status()\n",
    "            \n",
    "#             data = response.json()\n",
    "#             results = data['data']['results']\n",
    "#             all_results.extend(results)  # Almacena los resultados obtenidos\n",
    "#             return all_results\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error en la petici√≥n: {e}\")\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication in the Marvel API\n",
    "\n",
    "### Requirements for connecting to the Marvel API:\n",
    "To connect to the Marvel API, we must define our account's public and private keys in a `.env` file. These keys are:\n",
    "\n",
    "- **MARVEL_PUBLIC_KEY**: The public key provided by the API.\n",
    "- **MARVEL_PRIVATE_KEY**: The corresponding private key.\n",
    "\n",
    "### Connection Test\n",
    "\n",
    "The code below performs authentication with the Marvel API and checks if the connection is successful.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Obtain the keys from environment variables:**\n",
    "   We use `os.getenv` to get the API keys from the environment variables configured in the `.env` file. If the keys are not properly configured, the program will throw an error.\n",
    "\n",
    "2. **Generate the timestamp and hash for authentication:**\n",
    "   A `timestamp` is created with the current time, and an MD5 hash is generated by combining the `timestamp`, the private key, and the public key. This hash is required by the API to authenticate the requests.\n",
    "\n",
    "3. **Define the request parameters:**\n",
    "   The parameters `ts`, `apikey`, and `hash` are sent in each request to the API to ensure correct authentication.\n",
    "\n",
    "4. **Check the connection to the Marvel API:**\n",
    "   We use the `test_api_connection()` function to verify if the Marvel API is accessible and if authentication was successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the Marvel API.\n",
      "The Marvel API is accessible and the connection was successful.\n"
     ]
    }
   ],
   "source": [
    "# Requirements for making a request to the Marvel API:\n",
    "\n",
    "# Obtain the keys from environment variables\n",
    "public_key = os.getenv('MARVEL_PUBLIC_KEY')\n",
    "private_key = os.getenv('MARVEL_PRIVATE_KEY')\n",
    "\n",
    "if not public_key or not private_key:\n",
    "    raise ValueError(\"API keys are not configured in environment variables\")\n",
    "\n",
    "# Generate the timestamp using datetime\n",
    "ts = str(int(datetime.now().timestamp()))\n",
    "\n",
    "# Generate the MD5 hash\n",
    "hash_str = ts + private_key + public_key\n",
    "md5_hash = hashlib.md5(hash_str.encode('utf-8')).hexdigest()  # hexdigest() converts the hash to a hexadecimal representation, which is more readable for humans and commonly used in applications that require hashes.\n",
    "\n",
    "# Define params\n",
    "params = {\n",
    "    'ts': ts,\n",
    "    'apikey': public_key,\n",
    "    'hash': md5_hash\n",
    "}\n",
    "\n",
    "# Check the connection to the API:\n",
    "\n",
    "url_api = \"http://gateway.marvel.com/v1/public/characters\"\n",
    "\n",
    "if test_api_connection(url_api, params):\n",
    "    print(\"The Marvel API is accessible and the connection was successful.\")\n",
    "else:\n",
    "    print(\"Failed to connect to the Marvel API.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "### Process Description\n",
    "\n",
    "This code block performs incremental data extraction from the Marvel API, ensuring that the data is always up-to-date. We define the update frequency and then proceed to extract data on:\n",
    "\n",
    "1. **Marvel Characters**  \n",
    "2. **Marvel Comics**\n",
    "\n",
    "The date of the last extraction is stored and used to determine whether a new extraction needs to be executed. After each successful extraction, this date is updated for future incremental extractions.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Define the update frequency:**\n",
    "   Specify that the incremental extraction should be executed once per day.\n",
    "\n",
    "2. **Load the last extraction date:**\n",
    "   Read the date of the last extraction stored in a text file, which is used to check if enough time has passed to execute the next extraction.\n",
    "\n",
    "3. **Extract character data:**\n",
    "   Connect to the Marvel API and download character data. If the extraction is successful, the data is saved in Parquet format.\n",
    "\n",
    "4. **Extract comics related to characters:**\n",
    "   Obtain a list of comics in which each character appears and store the character-comic relationship in the Data Lake in Parquet format.\n",
    "\n",
    "5. **Extract comic data:**\n",
    "   Download comic information from the Marvel API and save it in Parquet format if new data is obtained.\n",
    "\n",
    "6. **Update the last extraction date:**\n",
    "   If new data was obtained, update the last extraction date in the corresponding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No updates\n"
     ]
    }
   ],
   "source": [
    "# Define how often the extraction will be updated\n",
    "\n",
    "# Path to store the last incremental extraction date\n",
    "LAST_EXTRACTION_DATE_FILE = \"datalake/bronze/marvel_api/last_extraction_date.txt\"\n",
    "\n",
    "# Get the date of the last incremental extraction\n",
    "last_extraction_date = get_last_extraction_date(LAST_EXTRACTION_DATE_FILE)\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Check if incremental extraction should run (once per day)\n",
    "should_run_incremental = last_extraction_date is None or (current_date - last_extraction_date).days >= 1\n",
    "\n",
    "# --------- Data Extraction --------- #\n",
    "\n",
    "url_api = \"http://gateway.marvel.com/v1/public\"\n",
    "\n",
    "# Data extraction of characters\n",
    "endpoint_characters = \"characters\"\n",
    "\n",
    "if should_run_incremental:\n",
    "    data_characters = get_data(url_api, endpoint_characters, params)\n",
    "    characters_df = build_table(data_characters)\n",
    "\n",
    "    if not characters_df.empty:\n",
    "        save_to_parquet(characters_df, \"datalake/bronze/marvel_api/characters/characters.parquet\")\n",
    "    \n",
    "    # Create a dataframe with a list of comics in which each character appears\n",
    "    # List to store the character-comics relationship\n",
    "    character_comics = []\n",
    "\n",
    "    # Iterate over each character in the characters results list\n",
    "    for character in data_characters:\n",
    "        # Step 1: Get the character ID\n",
    "        character_id = character['id']\n",
    "        \n",
    "        # Step 2: Get the list of comics in which the character appears\n",
    "        comics = character['comics']['items']\n",
    "        \n",
    "        # Step 3: Create an empty list to store the comic IDs\n",
    "        comic_ids = []\n",
    "        \n",
    "        # Step 4: Iterate over each comic in the comics list\n",
    "        for comic in comics:\n",
    "            # Extract the comic URI and split it into parts using '/' as a separator\n",
    "            parts = comic['resourceURI'].split('/')\n",
    "            \n",
    "            # Step 5: Get the last element of the 'parts' list, which is the comic ID\n",
    "            comic_id = parts[-1]\n",
    "            \n",
    "            # Step 6: Add the comic ID to the 'comic_ids' list\n",
    "            comic_ids.append(comic_id)\n",
    "        \n",
    "        # Step 7: Create a relationship between the character and their list of comics\n",
    "        relationship = {\n",
    "            'character_id': character_id,\n",
    "            'comic_ids': comic_ids\n",
    "        }\n",
    "        \n",
    "        # Step 8: Add the relationship to the general 'character_comics' list\n",
    "        character_comics.append(relationship)\n",
    "    \n",
    "    # Create the DataFrame from the list\n",
    "    character_comics_df = build_table(character_comics)\n",
    "\n",
    "    if not character_comics_df.empty:\n",
    "        save_to_parquet(character_comics_df, \"datalake/bronze/marvel_api/characters/character_comics.parquet\")\n",
    "\n",
    "# Data extraction of comics\n",
    "endpoint_comics = \"comics\"\n",
    "\n",
    "if should_run_incremental:\n",
    "    data_comics = get_comics(url_api, endpoint_comics, params)\n",
    "    comics_df = build_table(data_comics)\n",
    "\n",
    "    if not comics_df.empty:\n",
    "        save_to_parquet(comics_df, \"datalake/bronze/marvel_api/comics/comics.parquet\")\n",
    "\n",
    "# Update the last incremental extraction date if new data was obtained\n",
    "if should_run_incremental and not comics_df.empty:\n",
    "    last_extraction_date = datetime.now()\n",
    "    update_last_extraction_date(LAST_EXTRACTION_DATE_FILE, last_extraction_date)\n",
    "    # Print a message indicating that the extraction and data saving were completed\n",
    "    print(\"Extraction and data saving completed.\")\n",
    "else:\n",
    "    print(\"No updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Transformations:\n",
    "\n",
    "In this block, several transformations are performed on the data from the `characters` and `comics` tables obtained from the `bronze` layer of the data lake. These transformations include data loading, DataFrame merging, removal of duplicate and null values, optimization of data types for performance improvement, date manipulation, and renaming of columns for clarity. Finally, the transformed data is stored in the `silver` layer in Parquet format.\n",
    "\n",
    "1. **`characters` Table**:\n",
    "   - Load the Parquet files for `characters` and `character_comics`.\n",
    "   - Add a new column `character_comic_appearances` to the `characters` DataFrame by joining the DataFrames based on `character_id`.\n",
    "   - Filter the relevant columns for analysis.\n",
    "   - Replace empty strings with null values (`NaN`) and impute default values for null fields.\n",
    "   - Optimize numerical column types to smaller types (`int16`) to reduce memory usage, and convert some text columns to `categories`.\n",
    "   - Format the `modified` date column to `datetime`, handling invalid values and setting a default date for null values.\n",
    "   - Rename columns for better clarity.\n",
    "   - Finally, store the resulting DataFrame in the `silver` layer in Parquet format.\n",
    "\n",
    "2. **`comics` Table**:\n",
    "   - Load the Parquet file for the `comics` table.\n",
    "   - Filter the relevant columns for analysis.\n",
    "   - Replace empty strings with null values (`NaN`) and impute default values for null fields.\n",
    "   - Optimize numerical column types to smaller types (`int16` or `int32`) and convert some text columns to `categories`.\n",
    "   - Format the `modified` date column to `datetime`, handling invalid values and setting a default date for null values.\n",
    "   - Rename columns for better clarity.\n",
    "   - Finally, store the resulting DataFrame in the `silver` layer in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 10 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   character_id                 20 non-null     int64         \n",
      " 1   character_name               20 non-null     object        \n",
      " 2   description                  20 non-null     object        \n",
      " 3   thumbnail_path               20 non-null     object        \n",
      " 4   thumbnail_extension          20 non-null     category      \n",
      " 5   total_comics                 20 non-null     int16         \n",
      " 6   character_comic_appearances  20 non-null     object        \n",
      " 7   total_series                 20 non-null     int16         \n",
      " 8   total_stories                20 non-null     int16         \n",
      " 9   modified                     20 non-null     datetime64[ns]\n",
      "dtypes: category(1), datetime64[ns](1), int16(3), int64(1), object(4)\n",
      "memory usage: 9.5 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  characters_df['description'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  characters_df['comics.available'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  characters_df['series.available'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  characters_df['stories.available'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  comics_df['variantDescription'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:76: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  comics_df['description'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:77: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  comics_df['isbn'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  comics_df['upc'].replace('', np.nan, inplace=True)\n",
      "C:\\Users\\lucia\\AppData\\Local\\Temp\\ipykernel_10060\\1126786558.py:79: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  comics_df['diamondCode'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# -------------- Transformations df characters -------------- #\n",
    "\n",
    "# Fetch the characters dataframe\n",
    "characters_df = pd.read_parquet(\"datalake/bronze/marvel_api/characters/characters.parquet\")\n",
    "\n",
    "# Fetch the character_comics dataframe\n",
    "character_comics_df = pd.read_parquet(\"datalake/bronze/marvel_api/characters/character_comics.parquet\")\n",
    "\n",
    "# Add the 'comic_ids' column to the characters DataFrame by merging the DataFrames based on the 'character_id' column\n",
    "characters_df = characters_df.merge(character_comics_df, left_on='id', right_on='character_id', how='left')\n",
    "\n",
    "# Filter the columns of interest\n",
    "filter_columns = ['id', 'name', 'description', 'thumbnail.path', 'thumbnail.extension', 'comics.available', 'comic_ids', 'series.available', 'stories.available', 'modified']\n",
    "characters_df = characters_df[filter_columns]\n",
    "\n",
    "# Replace empty strings with NaN in the 'description', 'comics.available', etc. columns\n",
    "characters_df['description'].replace('', np.nan, inplace=True)\n",
    "characters_df['comics.available'].replace('', np.nan, inplace=True)\n",
    "characters_df['series.available'].replace('', np.nan, inplace=True)\n",
    "characters_df['stories.available'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Null value handling\n",
    "imputation_mapping = {\n",
    "    \"comics.available\": 0,\n",
    "    \"series.available\": 0,\n",
    "    \"stories.available\": 0,\n",
    "    \"description\": 'Description not available'\n",
    "}\n",
    "characters_df = characters_df.fillna(imputation_mapping)\n",
    "\n",
    "# Column type conversions, change the data type of numeric columns to a smaller type\n",
    "conversion_mapping = {\n",
    "    \"comics.available\": \"int16\",\n",
    "    \"series.available\": \"int16\",\n",
    "    \"stories.available\": \"int16\",\n",
    "    \"thumbnail.extension\": \"category\"\n",
    "}\n",
    "characters_df = characters_df.astype(conversion_mapping)\n",
    "\n",
    "# Date type conversions\n",
    "# From the 'modified' column, select only the first 10 characters which is just the date\n",
    "characters_df['modified'] = characters_df['modified'].str.slice(0, 10)\n",
    "# Convert 'modified' column to datetime, ignoring errors\n",
    "characters_df['modified'] = pd.to_datetime(characters_df['modified'], errors='coerce')\n",
    "# Replace NaT values with a predetermined date, which should be greater than 1970 for .parquet compatibility\n",
    "characters_df['modified'] = characters_df['modified'].fillna(pd.Timestamp('1971-01-01'))\n",
    "\n",
    "# Rename columns\n",
    "characters_df.rename(columns={\n",
    "    \"id\": \"character_id\",\n",
    "    \"name\": \"character_name\",\n",
    "    \"thumbnail.path\": \"thumbnail_path\",\n",
    "    \"thumbnail.extension\": \"thumbnail_extension\",\n",
    "    \"comics.available\": \"total_comics\",\n",
    "    \"comic_ids\": \"character_comic_appearances\",\n",
    "    \"series.available\": \"total_series\",\t\n",
    "    \"stories.available\": \"total_stories\"\n",
    "}, inplace=True)\n",
    "\n",
    "characters_df['character_comic_appearances'] = characters_df['character_comic_appearances'].astype(str)\n",
    "\n",
    "characters_df.info(memory_usage='deep')\n",
    "\n",
    "# Store data in parquet format in the silver layer\n",
    "if not characters_df.empty:\n",
    "    save_to_parquet(characters_df, \"datalake/silver/marvel_api/characters/characters.parquet\")\n",
    "\n",
    "# -------------- Transformations comics table -------------- #\n",
    "comics_df = pd.read_parquet(\"datalake/bronze/marvel_api/comics/comics.parquet\")\n",
    "comics_filter_columns = ['id', 'digitalId', 'title', 'issueNumber', 'variantDescription', 'description', 'isbn', 'upc', 'diamondCode', 'characters.items', 'stories.available', 'events.available', 'modified']\n",
    "comics_df = comics_df[comics_filter_columns]\n",
    "comics_df\n",
    "\n",
    "# Replace empty strings with NaN in the columns\n",
    "comics_df['variantDescription'].replace('', np.nan, inplace=True)\n",
    "comics_df['description'].replace('', np.nan, inplace=True)\n",
    "comics_df['isbn'].replace('', np.nan, inplace=True)\n",
    "comics_df['upc'].replace('', np.nan, inplace=True)\n",
    "comics_df['diamondCode'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Null value handling\n",
    "imputation_mapping = {\n",
    "    \"variantDescription\": 'Description not available',\n",
    "    \"description\": 'Description not available',\n",
    "    \"isbn\": 'No',\n",
    "    \"upc\": 'No',\n",
    "    \"diamondCode\": 'No'\n",
    "}\n",
    "comics_df = comics_df.fillna(imputation_mapping)\n",
    "\n",
    "# Column type conversions, change the data type of numeric columns to a smaller type\n",
    "conversion_mapping = {\n",
    "    \"id\": \"int32\",\n",
    "    \"digitalId\": \"int32\",\n",
    "    \"title\": \"category\",\n",
    "    \"description\": \"category\",\n",
    "    \"isbn\": \"category\",\n",
    "    \"upc\": \"category\",\n",
    "    \"diamondCode\": \"category\",\n",
    "    \"issueNumber\": \"int16\",\n",
    "    \"stories.available\": \"int16\",\n",
    "    \"events.available\": \"int16\"\n",
    "}\n",
    "comics_df = comics_df.astype(conversion_mapping)\n",
    "\n",
    "comics_df['characters.items'] = comics_df['characters.items'].astype(str)\n",
    "\n",
    "# Date type conversions\n",
    "# Convert 'modified' column to datetime, handling invalid values\n",
    "comics_df['modified'] = comics_df['modified'].str.slice(0, 10)\n",
    "# Convert 'modified' column to datetime, ignoring errors\n",
    "comics_df['modified'] = pd.to_datetime(comics_df['modified'], errors='coerce')\n",
    "# Replace NaT values with a predetermined date, which should be greater than 1970 for .parquet compatibility\n",
    "comics_df['modified'] = comics_df['modified'].fillna(pd.Timestamp('1970-01-01'))\n",
    "\n",
    "# Rename columns\n",
    "comics_df.rename(columns={\n",
    "    \"id\": \"comic_id\",\n",
    "    \"digitalId\": \"comic_digitalId\",\n",
    "    \"variantDescription\": \"variant_description\",\n",
    "    \"diamondCode\": \"diamond_code\",\n",
    "    \"issueNumber\": \"issue_number\",\t\n",
    "    \"stories.available\": \"total_stories\",\n",
    "    \"events.available\": \"total_events\",\n",
    "    \"characters.items\": \"characters_items\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Store data in parquet format in the silver layer\n",
    "if not comics_df.empty:\n",
    "    save_to_parquet(comics_df, \"datalake/silver/marvel_api/comics/comics.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading into AWS Redshift:\n",
    "\n",
    "This block handles loading the transformed data, located in the `silver` layer of the datalake, into an AWS Redshift database. The process includes the following steps:\n",
    "\n",
    "1. **Loading Data from the `silver` Layer**:\n",
    "   - The corresponding DataFrames for the `characters` and `comics` tables are loaded from Parquet files located in the datalake.\n",
    "\n",
    "2. **Accessing Redshift Credentials**:\n",
    "   - The credentials needed to connect to Redshift (host, database, user, password, and port) are obtained via environment variables.\n",
    "\n",
    "3. **Connecting to Redshift**:\n",
    "   - A connection to the Redshift database is established using `psycopg2`. If the connection is successful, a confirmation message is printed.\n",
    "\n",
    "4. **Creating Tables in Redshift**:\n",
    "   - SQL queries are defined and executed to create the `characters` and `comics` tables in Redshift, if they do not already exist. These tables include the transformed and optimized columns from the previous step.\n",
    "\n",
    "5. **Inserting Data**:\n",
    "   - A function is defined to insert data from the DataFrames into the Redshift tables. The data from each DataFrame is converted into a list of tuples and inserted into Redshift using dynamically generated SQL queries.\n",
    "\n",
    "6. **Closing the Connection**:\n",
    "   - The cursor and the connection to Redshift are closed, and a message is printed indicating that the data insertion has been completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conexion exitosa\n",
      "Inserci√≥n de datos en la base de datos Redshift completada.\n"
     ]
    }
   ],
   "source": [
    "characters_df = pd.read_parquet(\"datalake/silver/marvel_api/characters/characters.parquet\")\n",
    "comics_df = pd.read_parquet(\"datalake/silver/marvel_api/comics/comics.parquet\")\n",
    "\n",
    "# Access environment variables\n",
    "redshift_host = os.getenv('REDSHIFT_HOST')\n",
    "redshift_db = os.getenv('REDSHIFT_DB')\n",
    "redshift_user = os.getenv('REDSHIFT_USER')\n",
    "redshift_password = os.getenv('REDSHIFT_PASSWORD')\n",
    "redshift_port = os.getenv('REDSHIFT_PORT')\n",
    "\n",
    "# Establish connection with Redshift using psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_host,\n",
    "    dbname=redshift_db,\n",
    "    user=redshift_user,\n",
    "    password=redshift_password,\n",
    "    port=redshift_port\n",
    ")\n",
    "\n",
    "if conn:\n",
    "    print('successful connection')\n",
    "else:\n",
    "    print(\"Error connecting to Redshift\")\n",
    "\n",
    "# Create a cursor to execute queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Define queries to create tables in Redshift\n",
    "queries = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS public.characters (\n",
    "        character_id INT NOT NULL PRIMARY KEY,\n",
    "        character_name VARCHAR(255),\n",
    "        description VARCHAR(MAX),\n",
    "        thumbnail_path VARCHAR(255),\n",
    "        thumbnail_extension VARCHAR(50),\n",
    "        total_comics SMALLINT,\n",
    "        character_comic_appearances VARCHAR(MAX),\n",
    "        total_series SMALLINT,\n",
    "        total_stories SMALLINT,\n",
    "        modified TIMESTAMP\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS public.comics (\n",
    "        comic_id INT NOT NULL PRIMARY KEY,\n",
    "        comic_digitalId INT NOT NULL,\n",
    "        title VARCHAR(255),\n",
    "        issue_number SMALLINT,\n",
    "        variant_description VARCHAR(MAX),\n",
    "        description VARCHAR(MAX),\n",
    "        isbn VARCHAR(50),\n",
    "        upc VARCHAR(50),\n",
    "        diamond_code VARCHAR(50),\n",
    "        characters_items VARCHAR(MAX),\n",
    "        total_stories SMALLINT,\n",
    "        total_events SMALLINT,\n",
    "        modified TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Execute the queries to create the tables in Redshift\n",
    "for query in queries:\n",
    "    cur.execute(query)\n",
    "\n",
    "# Commit the operations\n",
    "conn.commit()\n",
    "\n",
    "# Function to insert data using psycopg2\n",
    "def insert_data(cursor, conn, df, table_name):\n",
    "    # Convert the DataFrame into a list of tuples\n",
    "    data_tuples = list(df.itertuples(index=False, name=None))\n",
    "    \n",
    "    # Generate the SQL for data insertion\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    columns = ', '.join(df.columns)\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Execute the insertion into Redshift\n",
    "    cursor.executemany(insert_query, data_tuples)\n",
    "    conn.commit()\n",
    "\n",
    "# Save the DataFrames to the Redshift database\n",
    "insert_data(cur, conn, characters_df, 'characters')\n",
    "insert_data(cur, conn, comics_df, 'comics')\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Print a message indicating the data insertion was completed\n",
    "print(\"Data insertion into Redshift database completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
